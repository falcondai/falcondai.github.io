{"componentChunkName":"component---src-gatsby-theme-blog-core-templates-post-query-js","path":"/blog/llm-robot-interaction/","result":{"data":{"site":{"siteMetadata":{"title":"Falcon Dai","author":"Falcon Dai","authorTwitter":"@falcondai","social":[{"name":"twitter","url":"https://twitter.com/falcondai"},{"name":"github","url":"https://github.com/falcondai"}]}},"blogPost":{"__typename":"MdxBlogPost","id":"60b60b0a-cdca-5109-8cb0-cdd243bc01cf","excerpt":"Due to the rapid empirical improvements in large language models (LLMs), there is a growing interests in robotics community to leverage them…","body":"var _excluded = [\"components\"];\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"Large Language Models-powered Human-Robotic Interactions\",\n  \"date\": \"2023-03-17T00:00:00.000Z\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Due to the rapid empirical improvements in large language models (LLMs), there is a growing interests in robotics community to leverage them for more natural human-robotic interactions.\"), mdx(\"p\", null, \"At a high level, there is a chasm between the natural language for a human user to express their instructions/goals/questions, such as English, and the \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"natural\"), \" language for a robot to execute, such as torque commands. This mismatch in native languages make a human-robot conversation difficult. The traditional approaches primarily focus on solving the subproblems, and often in specific environments, such as grounding/reference resolution and efficient motion planning. One frequent shortcoming in this line of approach is the very limited language that a robot can understand (think the small palette of inflexible commands that virtual assistants like Siri can resolve). In fact, the rigidly constructed commands that can be parsed often feel \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"robotic\"), \". In contrast, LLMs trained on enormous corpus, invite us to solve this challenge in a more holistic way.\"), mdx(\"h2\", {\n    \"id\": \"large-language-models\"\n  }, \"Large Language Models\"), mdx(\"p\", null, \"LLMs such as OpenAI GPT-3.5 are quite capable of code generation given just a textual description, such as programming problems. Instead of relying on a clever intermediate structured representation, LLMs are trained end-to-end to produce the final answer (by continuing the supplied prompts). Having been trained on the massive corpus covering many topics on the Internet, they seem to have implicitly captured an unprecedented amount of \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"common sense\"), \" that we share. This capability suggests a way forward beyond the tedious engineering of effectively domain-specific languages for each robot and each environment.\"), mdx(\"p\", null, \"For example, we can get Anthropic Claude v1.2 to brew and serve coffee from a suggestive prompt of incomplete Python snippet (zero-shot!).\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"from env import turn_on_coffee_machine, turn_off_coffee_machine, locate_cup\\nfrom cooking import resolve_ingredients\\nfrom two_arm_robot import grasp_object_left, grasp_object_right, place_object_left, place_object_right, plan_motion\\n\\n# A capable household robot.\\ndef brew_and_serve_coffee(coffee_type, kitchen_layout, serving_location):\\n\")), mdx(\"p\", null, \"The LLM completes it into:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"def brew_and_serve_coffee(coffee_type, kitchen_layout, serving_location):\\n    turn_on_coffee_machine()  # Turn on the coffee machine.\\n    ingredients = resolve_ingredients(coffee_type)  # Figure out the ingredients we need.\\n    cup_location = locate_cup(kitchen_layout)  # Find a cup.\\n    grasp_object_left(cup_location)  # Grasp the cup with the left arm.\\n\\n    for ingredient in ingredients:\\n        ingredient_location = locate_object(ingredient, kitchen_layout)  # Find the ingredient.\\n        grasp_object_right(ingredient_location)  # Grasp the ingredient container with the right arm.\\n        place_object_left(ingredient_location)  # Pour the ingredient into the cup.\\n        grasp_object_right(ingredient_location)  # Put the ingredient container back.\\n\\n    place_object_left(serving_location)  # Place the full cup at the serving location.\\n    turn_off_coffee_machine()  # Turn off the coffee machine.\\n\")), mdx(\"p\", null, \"Without definitions of the unlying functions, it is not clear that this will work but it clearly demonstrates that the LLM knows about coffee making in a kitchen at some level of details and could express it in Python (in terms of controlling a two-armed robot).\"), mdx(\"h2\", {\n    \"id\": \"approach-in-code-as-policies\"\n  }, \"Approach in Code-As-Policies\"), mdx(\"p\", null, \"An interesting recent work attempts to bridge this gap with LLM is \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://code-as-policies.github.io\"\n  }, \"code-as-policies\"), \". This work has some attractive features: it uses a trained LLM without any extra training and LLM generates executable Python code for the robot. But an LLM such as GPT-3.5 cannot perceive an image directly. How can we specialize its generation for a specific situation?\"), mdx(\"p\", null, \"Main ideas:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Via prompt engineering, we can reduce the problem into format akin to programming challenges, a domain where LLM has showed some success.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"We implement (motion) primitives for the environment and related basic functions that serve the motion primitives. They are responsible for the low-level robot control. For example, in the demo below, the only motion primitive is picking from one position and then placing at another position on a tabletop. We also need to implement functions that returns an object (identified by its ID)'s position to support this primitive. These APIs effectively translate information from different sensing modalities into code which LLM can read. And as a whole, they implicitly define an ontology for the environment.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"With a few examples involving each primitives (few-shot prompting), we show LLM how this specific robot and environment works.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"LLM does the heavy lifting of grounding (resolving references) and planning which turns instructions into primitives. Interestingly, the solutions can contain unimplemented functions with function names suggestive of their utility. We leverage the \\\"common sense\\\" encoded in LLM to bypass some of the hardest subproblems in robotics, though not without silly mistakes at times.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Whenever we encounter an undefined function in the generated code, we prompt the LLM to generate its implementation.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Question & answering, conversational capabilities naturally arise via a few appropriate examples and passing past messages in prompts, respectively. Again, no specialized algorithms are needed. A trained LLM already comes with some ability to reply to questions (and generate code that finds the answer) and use contexts.\")), mdx(\"h2\", {\n    \"id\": \"some-observations\"\n  }, \"Some Observations\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"GPT-3.5 seems quite capable in reducing natural language description related to positions into arithmetics of the coordinates.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Despite not being a code generation-centric model, \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"text-davinci-003\"), \" tends to output better solutions than \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"code-davinci-002\"), \".\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Recursive function generation is a nice idea but it is quite error-prone in practice, e.g., mismatched semantics between a function's usage and its implementation.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"The solution is very much top-down functionally speaking. LLM has to come up a solution that covers all kinds of possible situations. This seems more complex than necessary: we only need a solution that solves the problem in the \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"current\"), \" situation.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"LLM can generate code involving objects not in the scene and that gives error. Generated code can contain syntax errors and it is messy to execute strings as Python code.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Prompts can grow pretty long quickly due to the many demonstrative examples and the growing conversational history. This worsens latency and creates a upper limit on the conversational length. How can we maintain a more concise context?\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Nevertheless I am impressed by how far this solution gets us (in this tabletop environment demo). I also cannot help but think that some of the observed mistakes can be remedied through prompt engineering, e.g., providing better demonstrative examples, adding more concepts. But at some point, will prompt engineering be as tedious as the traditional decompositional solution? The former is an extensional definition whereas the latter, intensional. When is it a good idea to use the latter? Is there some way to combine them?\")), mdx(\"h2\", {\n    \"id\": \"demo\"\n  }, \"Demo\"), mdx(\"p\", null, \"Building on the original demo, I improved the UI and added shortcuts for experimenting with prompting techniques. Chain-of-thought prompting (by asking LLM for a step-by-step explanation) tends to produce a better solution. Explicitly asking LLM to request clarifications could trigger more conversational behaviors when our instructions contain ambiguity.\"), mdx(\"iframe\", {\n    src: \"https://falcondai-code-as-policies.hf.space\",\n    frameBorder: \"0\",\n    width: \"850\",\n    height: \"1600\"\n  }));\n}\n;\nMDXContent.isMDXComponent = true;","slug":"/blog/llm-robot-interaction/","title":"Large Language Models-powered Human-Robotic Interactions","tags":[],"keywords":[],"date":"March 17, 2023"},"previous":{"__typename":"MdxBlogPost","id":"0ff02689-f05b-5a30-978c-84da34a68ac8","excerpt":"As many schools moved classes online during the COVID-19 outbreak, I think a review of solutions for remote instructions will be helpful. I…","slug":"/blog/remote-teaching-ipad/","title":"Remote Teaching with an iPad","date":"March 16, 2020"},"next":null},"pageContext":{"id":"60b60b0a-cdca-5109-8cb0-cdd243bc01cf","previousId":"0ff02689-f05b-5a30-978c-84da34a68ac8"}},"staticQueryHashes":["4198970465","880678893"]}